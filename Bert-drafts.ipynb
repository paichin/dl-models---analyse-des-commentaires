{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "\n",
    "!wget -q https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
    "!unzip -o chinese_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pretrained_path = 'chinese_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "\n",
    "# TF_KERAS must be added to environment variables in order to use TPU\n",
    "# os.environ['TF_KERAS'] = '1'\n",
    "\n",
    "bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n",
    "\n",
    "for l in bert_model.layers:\n",
    "    l.trainable = True\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_reviews=pd.read_csv('Train_reviews.csv')\n",
    "train_labels=pd.read_csv('Train_labels.csv')\n",
    "\n",
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#O,B-opinion,I-opinion,B-aspect,I-aspect\n",
    "output_res=[]\n",
    "for id_ in tqdm(range(len(train_reviews))):#5改成句子总数\n",
    "    to_cut=train_reviews.iloc[id_]['Reviews']\n",
    "    list_cut=[]\n",
    "    for i in to_cut:\n",
    "        list_cut.append(i)\n",
    "    \n",
    "    a_list,o_list=[],[]\n",
    "    b_set=set()#形容词或名词的开头位置\n",
    "    temp_labels=train_labels[train_labels['id']==id_+1].reset_index(drop=True)\n",
    "    for i in range(len(temp_labels)):\n",
    "        temp=temp_labels.iloc[i]#一行记录\n",
    "        if temp['AspectTerms']!='_':\n",
    "            b_set.add(temp['A_start'])\n",
    "            for num in range(int(temp['A_start']),int(temp['A_end'])):\n",
    "                a_list.append(num)\n",
    "        if temp['OpinionTerms']!='_':\n",
    "            b_set.add(temp['O_start'])\n",
    "            for num in range(int(temp['O_start']),int(temp['O_end'])):\n",
    "                o_list.append(num)\n",
    "    pivot=0\n",
    "    last_tag='O'\n",
    "    \n",
    "    for i in range(len(list_cut)):\n",
    "        in_a = False\n",
    "        in_o = False\n",
    "        tag='O'\n",
    "        if (pivot in a_list):\n",
    "            in_a = True\n",
    "            if (((last_tag=='B-aspect') | (last_tag=='I-aspect')) & (str(pivot) not in b_set)):\n",
    "                tag='I-aspect'\n",
    "            else:\n",
    "                tag='B-aspect'\n",
    "        if (pivot in o_list):\n",
    "            in_o = True\n",
    "            if (((last_tag=='B-opinion') | (last_tag=='I-opinion')) & (str(pivot) not in b_set)):\n",
    "                tag='I-opinion'\n",
    "            else:\n",
    "                tag='B-opinion'\n",
    "        output_res.append([id_+1,tag,list_cut[i]])\n",
    "        last_tag=tag\n",
    "        pivot+=len(list_cut[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_train=pd.DataFrame(output_res)\n",
    "data_train.columns=['Sentence #','Tag','Word']\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_in = Input(shape=(None,))#embedding去掉，启用bert\n",
    "x2_in = Input(shape=(None,))\n",
    "\n",
    "x = bert_model([x1_in, x2_in])\n",
    "\n",
    "x = TimeDistributed(Dense(50, activation=\"tanh\"))(x)\n",
    "crf = CRF(6, sparse_target=False)\n",
    "x = crf(x)\n",
    "model = Model([x1_in, x2_in], x)\n",
    "\n",
    "decay_steps, warmup_steps = calc_train_steps(\n",
    "    #train_y.shape[0],\n",
    "    2600,\n",
    "    batch_size=16,\n",
    "    epochs=1,\n",
    ")\n",
    "\n",
    "LR=5e-5\n",
    "\n",
    "model.compile(\n",
    "    #optimizer=Adam(learning_rate),\n",
    "    optimizer=AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR),\n",
    "    loss=crf.loss_function, \n",
    "    metrics=[crf.accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Convert Data to Array\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras_bert import Tokenizer\n",
    "\n",
    "SEQ_LEN=75\n",
    "\n",
    "tag2idx = {}\n",
    "tag2idx['[PAD]'] = [1,0,0,0,0,0]\n",
    "tag2idx['O'] = [0,1,0,0,0,0]\n",
    "tag2idx['B-aspect'] = [0,0,1,0,0,0]\n",
    "tag2idx['I-aspect'] = [0,0,0,1,0,0]\n",
    "tag2idx['B-opinion'] = [0,0,0,0,1,0]\n",
    "tag2idx['I-opinion'] = [0,0,0,0,0,1]\n",
    "\n",
    "class OurTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]') # space类用未经训练的[unused1]表示\n",
    "            else:\n",
    "                R.append('[UNK]') # 剩余的字符是[UNK]\n",
    "        return R\n",
    "\n",
    "tokenizer = OurTokenizer(token_dict)\n",
    "\n",
    "def load_data():\n",
    "    global tokenizer\n",
    "    indices, seqs = [], []\n",
    "    for i in range(len(train_reviews)):#句子id-1\n",
    "        id_ = i+1\n",
    "        text = train_reviews.iloc[i]['Reviews']\n",
    "        ids, segments = tokenizer.encode(text, max_len=SEQ_LEN)\n",
    "        tag_list = list(data_train[data_train['Sentence #']==id_]['Tag'])\n",
    "        seq = [[0,1,0,0,0,0]]\n",
    "        seq = seq + ([tag2idx[k] for k in tag_list])\n",
    "        for j in range(SEQ_LEN-len(seq)):\n",
    "            seq.append([1,0,0,0,0,0])\n",
    "        seq = np.array(seq)\n",
    "        indices.append(ids)\n",
    "        seqs.append(seq)\n",
    "    items = list(zip(indices, seqs))\n",
    "    #np.random.shuffle(items)\n",
    "    indices, seqs = zip(*items)\n",
    "    indices = np.array(indices)\n",
    "    return [indices, np.zeros_like(indices)], np.array(seqs)\n",
    "  \n",
    "def load_test_data():\n",
    "    global tokenizer\n",
    "    indices = []\n",
    "    for i in range(len(test_reviews)):#句子id-1\n",
    "        id_ = i+1\n",
    "        text = test_reviews.iloc[i]['Reviews']\n",
    "        ids, segments = tokenizer.encode(text, max_len=SEQ_LEN)\n",
    "        indices.append(ids)\n",
    "    indices = np.array(indices)\n",
    "    return indices#[indices, np.zeros_like(indices)]\n",
    "  \n",
    "test_reviews=pd.read_csv('Test_reviews.csv')\n",
    "train_x, train_y = load_data()\n",
    "test_x = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试时\n",
    "x_train=train_x[0][0:2600,:]\n",
    "y_train=train_y[0:2600,:,:]\n",
    "x_test=train_x[0][2600:,:]\n",
    "y_test=train_y[2600:,:,:]\n",
    "\n",
    "#生成结果时\n",
    "# x_train=train_x[0]\n",
    "# y_train=train_y\n",
    "# x_test=test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    [x_train,np.zeros(x_train.shape)],\n",
    "    y_train,\n",
    "    epochs=4,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = model.predict([x_test,np.zeros(x_test.shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_idx0 = [[np.argmax(k,axis=0) for k in i] for i in pre]\n",
    "pre_idx_test = [[np.argmax(k,axis=0) for k in i] for i in y_test]\n",
    "\n",
    "cnt=0\n",
    "same=0\n",
    "for i in range(len(pre_idx0)):\n",
    "  cnt+=1\n",
    "  if pre_idx0[i]==pre_idx_test[i]:\n",
    "    same+=1\n",
    "print(same*1.0/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_idx0 = [[np.argmax(k,axis=0) for k in i] for i in pre]\n",
    "\n",
    "\n",
    "idx2tag={}\n",
    "idx2tag[0]='[PAD]'\n",
    "idx2tag[1]='O'\n",
    "idx2tag[2]='B-aspect'\n",
    "idx2tag[3]='I-aspect'\n",
    "idx2tag[4]='B-opinion'\n",
    "idx2tag[5]='I-opinion'\n",
    "pre_idx = [[idx2tag[j]for j in i] for i in pre_idx0]\n",
    "\n",
    "test_data=test_reviews\n",
    "\n",
    "test_review_list = []\n",
    "for to_cut in tqdm(test_data['Reviews']):\n",
    "    s_list=[]\n",
    "    for i in to_cut:\n",
    "      s_list.append(i)\n",
    "    test_review_list.append(s_list)\n",
    "\n",
    "p_all_tags_new=[]\n",
    "for i in tqdm(range(len(pre_idx))):\n",
    "    l_sentence=len(test_review_list[i])+1\n",
    "    l_new=pre_idx[i][0:l_sentence]\n",
    "    l_temp=[s if s!='[PAD]' else 'O' for s in l_new]\n",
    "    l_temp=l_temp[1:len(l_temp)]\n",
    "    p_all_tags_new.append(l_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rtn_AO_list(s_id,l_word,l_tag):\n",
    "    res=[]\n",
    "    l_ao_tag=[]\n",
    "    l_ao_phrase=[]\n",
    "    s=''\n",
    "    for i in range(len(l_word)):\n",
    "        if l_tag[i]=='O':\n",
    "            s=''\n",
    "        elif l_tag[i]=='B-aspect':\n",
    "            l_ao_tag.append('A')\n",
    "            if (i+1)==len(l_word):\n",
    "                l_ao_phrase.append(l_word[i])\n",
    "            elif l_tag[i+1]!='I-aspect':\n",
    "                l_ao_phrase.append(l_word[i])\n",
    "            else:\n",
    "                s=l_word[i]\n",
    "        elif l_tag[i]=='B-opinion':\n",
    "            l_ao_tag.append('O')\n",
    "            if (i+1)==len(l_word):\n",
    "                l_ao_phrase.append(l_word[i])\n",
    "            elif l_tag[i+1]!='I-opinion':\n",
    "                l_ao_phrase.append(l_word[i])\n",
    "            else:\n",
    "                s=l_word[i]\n",
    "        else:\n",
    "#这段代码是把这个为I而上一个为O的，把O+I一起输出为这个元素,这样会导致准确率降低召回率升高\n",
    "#             if i>=1:#\n",
    "#                 if l_tag[i-1]=='O':#\n",
    "#                     if l_tag[i]=='I-aspect':#\n",
    "#                         l_ao_tag.append('A')#\n",
    "#                         s=l_word[i-1]#\n",
    "#                     else:#\n",
    "#                         l_ao_tag.append('O')#\n",
    "#                         s=l_word[i-1]#\n",
    "            if (i+1)==len(l_word):\n",
    "                l_ao_phrase.append(s+l_word[i]) \n",
    "            elif ((l_tag[i+1]!='I-aspect') & (l_tag[i+1]!='I-opinion')):\n",
    "                if l_tag[i-1]!='O':#这个if是和上面相反的策略，即如果I前不是B则不输出\n",
    "                    l_ao_phrase.append(s+l_word[i])\n",
    "            else:\n",
    "                s+=l_word[i]\n",
    "    container_tag=''\n",
    "    container_phrase=''\n",
    "    len0=len(l_ao_tag)\n",
    "    if len0>0:\n",
    "        for i in range(len0):\n",
    "            if container_tag=='':\n",
    "                container_tag=l_ao_tag[i]\n",
    "                container_phrase=l_ao_phrase[i]\n",
    "            elif container_tag=='A':\n",
    "                if l_ao_tag[i]=='O':\n",
    "                    res.append([s_id,container_phrase,l_ao_phrase[i]])\n",
    "                    container_tag,container_phrase='',''\n",
    "                else:\n",
    "                    res.append([s_id,container_phrase,'_'])\n",
    "                    container_phrase=l_ao_phrase[i]\n",
    "            elif container_tag=='O':\n",
    "                if l_ao_tag[i]=='A':\n",
    "                    res.append([s_id,l_ao_phrase[i],container_phrase])\n",
    "                    container_tag,container_phrase='',''\n",
    "                else:\n",
    "                    res.append([s_id,'_',container_phrase])\n",
    "                    container_phrase=l_ao_phrase[i]\n",
    "        if container_tag!='':\n",
    "            if container_tag=='A':\n",
    "                res.append([s_id,container_phrase,'_'])\n",
    "            else:\n",
    "                res.append([s_id,'_',container_phrase])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "res=[]\n",
    "for sentence_index in tqdm(range(len(test_review_list))):\n",
    "    s_id=list(test_data['id'])[sentence_index]\n",
    "    l_word,l_tag=[],[]\n",
    "    for word_index in range(len(test_review_list[sentence_index])):\n",
    "        this_word=test_review_list[sentence_index][word_index]\n",
    "        if len(re.findall(',|，|。|、|\\.|!|！|\\?|？|；|;| ',this_word))==0:\n",
    "            l_word.append(this_word)\n",
    "            l_tag.append(p_all_tags_new[sentence_index][word_index])######\n",
    "        else:\n",
    "            res+=rtn_AO_list(s_id,l_word,l_tag)\n",
    "            l_word,l_tag=[],[]\n",
    "    res+=rtn_AO_list(s_id,l_word,l_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res=pd.DataFrame(res)\n",
    "df_res['tuple']=df_res.apply(lambda x:(x[0],x[1],x[2]),axis=1)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true=pd.read_csv('Train_labels.csv')\n",
    "df_true=df_true[df_true['id']>2600]\n",
    "df_true=df_true[['id','AspectTerms','OpinionTerms']]\n",
    "df_true['tuple']=df_true.apply(lambda x:(x[0],x[1],x[2]),axis=1)\n",
    "c=0\n",
    "for i in df_res['tuple']:\n",
    "    if i in set(df_true['tuple']):\n",
    "        c+=1\n",
    "precision=c/len(df_res)\n",
    "recall=c/len(df_true)\n",
    "print(c)\n",
    "print(precision)\n",
    "print(recall)\n",
    "2*precision*recall/(precision+recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
